\section{Appendix}

\subsection{National Material Capabilities Data}

Our predictors are taken from the National Material Capabilities (v4.0) dataset from the Correlates of War project \citep{singer1972}.\footnote{
  Downloaded from \url{http://correlatesofwar.org/data-sets/national-material-capabilities/nmc-v4-data/at_download/file}.
}
The dataset contains observations on six variables for 14,199 country-years from 1816 to 2007.
For details on the variables and their measurement, see the NMC Codebook.\footnote{
  Available at \url{http://correlatesofwar.org/data-sets/national-material-capabilities/nmc-codebook/at_download/file}.
}
Table~\ref{tab:summary} lists the proportions of zeroes and missing values among each variable.

\begin{table}[htp]
  \centering
  \input{tab-summary}
  \caption{
    Proportions of zeroes and missing values in each National Military Capability component variable.
  }
  \label{tab:summary}
\end{table}

All six variables are strongly right-skewed.
Since five of the six variables are sometimes zero-valued (though all are non-negative), a logarithmic transformation is not appropriate.
Instead, to correct for skewness, we apply an inverse hyperbolic sine transformation \citep{Burbidge:1988gu} to each component:
\begin{equation}
  \label{eq:asinh}
  h(x, \theta)
  =
  \sinh^{-1} (\theta x)
  =
  \log \left(
    \theta x + \sqrt{(\theta x)^2 + 1}
  \right).
\end{equation}
We set the scale~$\theta$ separately for each component variable with the aim of making the transformed variable approximately normally distributed.
Specifically, for each variable, we choose the value of $\theta \in \{2^d\}_{d=-10}^{10}$ that minimizes the Kolmogorov--Smirnov test statistic \citep{MasseyJr:2012jo} against a normal distribution with the same mean and variance.
Table~\ref{tab:summary} gives the scale selected for each component.
We use the transformed components in both the multiple imputation (see below) and the super learner training.

\subsection{Militarized Interstate Dispute Data}

Our sample and outcome variable are taken from the Militarized Interstate Disputes (v4.1) dataset from the Correlates of War project \citep{Palmer:2015hp}.\footnote{
  Downloaded from \url{http://correlatesofwar.org/data-sets/MIDs/mid-level/at_download/file}.
}
The dataset records the participants and outcomes of interstate disputes from 1816 to 2010.
To avoid the problem of aggregating capabilities across multiple states, we exclude disputes with more than one state on either side.
We drop disputes that end in an outcome other than one side winning, one side yielding, or a stalemate;\footnote{
  For details on other kinds of outcomes, see the MID Codebook.
}
we then collapse ``A Wins'' and ``B Yields'' into a single coding, and similarly for ``B Wins'' and ``A Yields.''
Finally, since the capabilities data only run through 2007, we exclude disputes that end after 2007.
In the end, we have $N = 1{,}740$ cases.
Table~\ref{tab:mid} lists the distribution of outcomes in the full data and in the split subsamples.

\begin{table}[htp]
  \centering
  \input{tab-mid}
  \caption{
    Distribution of the three dispute outcomes in the full dataset, the training subset, and the test subset.
  }
  \label{tab:mid}
\end{table}

For each dispute in our dataset, we code the participating countries' capabilities using the values in the year the dispute began.
About 17~percent of disputes have at least one missing capability component for at least one participant.

\subsection{Multiple Imputation}

As noted above, all of the National Material Capabilities variables contain some missing values.
Following standard practice, we multiply impute the missing observations \citep{honaker_what_2010}.
We perform the imputations via the \texttt{Amelia} software package \citep{pkg-Amelia}.

Rather than just impute the missing values in the final dataset of disputes, we impute the entire National Material Capabilities dataset.
This allows us to fully exploit the dataset's time-series cross-sectional structure in the imputation process.
We include in the imputation model a cubic polynomial for time, interacted with country dummy variables.
As this results in an explosion in the number of parameters in the imputation model, we then impose a ridge prior equal to 0.1 percent of the observations in the dataset (see Section~4.7.1 of the \texttt{Amelia} package vignette).
We enforce the constraint that every imputed value be non-negative.
Finally, we impose an observation-level prior with mean zero and variance equal to that of the observed values of the corresponding component variable for every missing cell that meets the following criteria:
\begin{itemize}
  \item There are no non-zero observed values in the time series preceding the cell
  \item The first observed value that comes after the cell is zero
\end{itemize}
So, for example, if a country's urban population is zero from 1816 to 1840, missing from 1841 to 1849, and zero in 1850, we would impose this form of prior on the 1841--1849 values.
Diagnostic time series plots of observed versus imputed values within each data series, generated by the \texttt{tscsPlot()} function in \texttt{Amelia}, are available on request.

The presence of missing data also complicates the calculations of country-by-country proportions of the total amount of each component by year.
One option is to recompute the annual totals in each imputed dataset, so that the resulting data will be logically consistent---in particular, all proportions will sum to one.
The drawback of this approach is that virtually every observation of the proportions will differ across the imputed datasets, even for countries with no missing data, since the annual totals will differ across imputations.
An alternative approach is to compute the annual totals using only the observed values.
The advantage is that non-missing observations will not vary across imputed datasets; the downside is that the proportions within each imputation will generally sum to more than one.
For our purposes in this paper, we think it is preferable to reduce variation across imputations, even at the expense of some internal consistency in the imputed datasets, so we take the latter approach: annual totals are the sums of only the observed values.

We impute $I = 10$ datasets of national capabilities according to the procedure laid out above, and we merge each with the training subset of our dispute data to yield $I$ training data imputations.
We run the super learner separately on each imputation, and our final model is an (unweighted) average of the $I$ super learners.

After training is complete, we run into missing data problems once again when calculating DOE scores.
To calculate predicted probabilities for dyads with missing values, we calculate a \emph{new} set of $I = 10$ imputations of the capabilities data and take an (unweighted) average of our model's predictions across the imputations.

\subsection{Super Learner Candidate Models}

We use the R statistical environment \citep{pkg-R} for all data analysis.
We fit, cross-validate, and calculate predictions from each candidate model through the \texttt{caret} package \citep{pkg-caret}.
We then construct the super learner by solving~\eqref{eq:super-learner} via the \texttt{constrOptim()} function for optimization with linear constraints in base R.
Further details about each candidate model are summarized below.

\begin{itemize}
  \item Ordered Logistic Regression
  \begin{description}
    \item[Package] \texttt{MASS} \citep{pkg-MASS}
    \item[Tuning Parameters] None
    \item[Notes] In the ``Year'' models, the year of the dispute is included directly and interacted with each capability variable
  \end{description}

  \item $k$-Nearest Neighbors
  \begin{description}
    \item[Package] \texttt{caret} \citep{pkg-caret}
    \item[Tuning Parameters] ~
    \begin{itemize}
      \item Number of nearest neighbors to average (\texttt{k}): selected via cross-validation from $\{25, 50, \ldots, 250\}$
    \end{itemize}
    \item[Notes] All predictors centered and scaled to have zero mean and unit variance
  \end{description}

  \item Random Forest
  \begin{description}
    \item[Package] \texttt{randomForest} \citep{pkg-randomForest}
    \item[Tuning Parameters] ~
    \begin{itemize}
      \item Number of predictors randomly sampled at each split (\texttt{mtry}): selected via cross-validation from $\{2, 4, \ldots, 12\}$ for models without year and from $\{2, 3, 5, \ldots, 13\}$ for models with year
    \end{itemize}
    \item[Notes] 1,000 trees per fit
  \end{description}

  \item Neural Network
  \begin{description}
    \item[Package] \texttt{nnet} \citep{pkg-MASS}
    \item[Tuning Parameters] ~
    \begin{itemize}
      \item Number of hidden layer units (\texttt{size}): selected via cross-validation from $\{1, 3, 5, 7, 9\}$
      \item Weight decay parameter (\texttt{decay}): selected via cross-validation from $\{10^{-d}\}_{d=0}^4$
    \end{itemize}
    \item[Notes] Single hidden layer, no skip layer, softmax fitting
  \end{description}

  \item Gaussian Process
  \begin{description}
    \item[Package] \texttt{kernlab} \citep{pkg-kernlab}
    \item[Tuning Parameters] ~
    \begin{itemize}
      \item Kernel width (\texttt{sigma}): selected automatically via \texttt{sigest()} in the \texttt{kernlab} package
    \end{itemize}
    \item[Notes] Radial basis kernel, all predictors centered and scaled to have zero mean and unit variance
  \end{description}

  \item Support Vector Machine
  \begin{description}
    \item[Package] \texttt{kernlab} \citep{pkg-kernlab}
    \item[Tuning Parameters] ~
    \begin{itemize}
      \item Kernel width (\texttt{sigma}): selected automatically via \texttt{sigest()} in the \texttt{kernlab} package
      \item Constraint violation cost (\texttt{C}): selected via cross-validation from $\{2^d\}_{d=-2}^{12}$
    \end{itemize}
    \item[Notes] Radial basis kernel, all predictors centered and scaled to have zero mean and unit variance
  \end{description}
\end{itemize}

\subsection{Replications}

The following list contains basic information about each model in the replication study.
We carry out logistic and probit regressions via \texttt{glm()} in base R \citep{pkg-R}, multinomial logit via \texttt{multinom()} in the \texttt{nnet} package \citep{pkg-MASS}, ordered probit via \texttt{polr()} in the \texttt{MASS} package \citep{pkg-MASS}, and heteroskedastic probit via \texttt{hetglm()} in the \texttt{glmx} package \citep{pkg-glmx}.

\input{list-replications}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "doe"
%%% End:
