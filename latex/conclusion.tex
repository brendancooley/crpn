\section{Conclusion}
In this paper, we have argued that proxies should be constructed using predictive power as the criterion of interest, provided a method for doing so, and demonstrated the usefulness of the method in an application to measuring dispute outcomes.  We hope that our efforts will be of use both for both the DOE scores we provide and for the theoretical merits of our general argument.

In our application, the DOE scores outperform the extant proxy---the CINC-based capability ratio---in a number of important ways.  In pure terms, the DOE score more closely relates to what international relations care about:  the expected outcome of a dispute between two nations.  It therefore has a much more natural interpretation than the capability ratio.  It also lacks the \emph{ad hoc} assumptions imposed by both the CINC score and the ratio-based transformation used in most studies.  On the practical side, our replications suggest that the DOE score is a better contributor to the usual battery of variables included in the ever-expanding universe of international relations regressions.  We hope, then, that it will find use as scholars advance and test new claims.  

Though it represents a massive improvement over the \emph{status quo}, the DOE score could still be improved.  We have only included those variables that could be extracted from the data used to construct the capability ratio---namely, the six Correlates of War National Material Capabilities variables.  We did so consciously, as we wanted to demonstrate that our general approach could improve measures without introducing new covariates.  Having made our point, we look forward to seeing what the future holds for coming versions of DOE when new data is brought to bear on the problem.  At the risk of belaboring:  we created DOE using open-source software and have made our replication code available, and so anybody with a computer---and some patience!---could create a new version with new covariates.

On the theoretical side, we believe that our data-driven approach to measurement will prove useful for those wishing to proxy for other quantities.  All one needs is a set of predictor variables $\boldsymbol{x}$ and some outcome of interest $\boldsymbol{y}$---the $f$ we provide to map $\boldsymbol{x}$ to $\boldsymbol{y}$ will work.  Just as with introducing new covariates in any given application, future scholars can improve their proxies by including new models for evaluation in the super learner.  The general approach, however, remains unchanged.  Our application tasked us to create a proxy of a probabilistic expectation like those seen in formal models of choice under uncertainty, and similar applications provide a natural starting point for our method.  Doing so, however, requires good theory for just what it is that we hope to predict with our abstractions---for example, what outcome could we use variables related to democracy, like those used in the Polity score \citep{marshall2014}, to predict?  Hopefully readers from other subfields already have examples springing into their heads even as they read this sentence.  We ask them to turn their attention to these examples in the coming years.

We would like to conclude with a still broader point.  \citet{Breiman:2001fd} argues that statistical modelers fall into one of two cultures:  data modelers, who interpret models' estimates after assessing overall quality via in-sample goodness of fit; and algorithmic modelers, who seek algorithms that predict responses as well as possible given some set of covariates.\footnote{In case it is not obvious from our previous citations, Breiman self-identifies to the second culture.  He argues that 98\% of statisticians fall into the data modeling camp, which is a point we lack the expertise to assess.  However, we are comfortable saying that the vast majority of orthodox, empirical political scientists are data modelers.}  The method we advance is certainly algorithmic.  Our decision to adopt algorithmic modeling based on prediction, however, was not culture-driven---it was purpose-driven \citep{clarke2012}.  Most simply, many quantities to be proxied for are expectations, so they should be constructed with prediction in mind.  However, we realize that most political scientists remain, and will continue to remain, firmly entrenched in the data modeling culture.  Even now, in the middle of the identification revolution where analysts care more and more about the size of causal effects and less and less about traditional hypothesis testing, most empirically-minded political scientists---ourselves included---often find themselves interested in some hypothesis best assessed via data modeling.  This is why we took the DOE score's in- and out-of-sample performance in the replications so seriously:  despite its origins within the algorithmic culture, we want DOE to be of use to those who plan to remain in the data modeling culture.  Real people often reflect influences from seemingly contradictory cultures; we, as political scientists, are just as pleased that the DOE score performs better in regressions representative of our field as we are that it predicts better out out sample.  As new problems emerge and new solutions arise to solve them, we believe methodological pragmatism will be an important virtue.

Regardless of one's viewpoint, however, it remains that our discipline is measured in large part on the quality of its measures, and our approach has created at least one that performs better on a number of dimensions.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "doe"
%%% End:
