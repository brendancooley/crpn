%!TEX root = doe.tex

One of the greatest challenges in political science is to measure theoretical quantities.
Because important concepts like ideology, democracy, and power cannot be measured directly, measurement problems in political science often entail the construction of proxies.
Recent advances in computing and modeling have allowed political scientists to build sophisticated, data-driven proxies for variables as diverse as legislator ideology \citep{clinton2004}, judicial independence \citep{linzer2014}, and country regime types \citep{jackman2008}.
Yet there is no consensus on what makes for a good proxy, nor a common metric by which proxy variables are evaluated.
Moreover, there has been relatively little progress improving our measurements of variables that do not fit the item-response paradigm.

In this article, we argue for a predictive criterion for proxy quality.
If the variable of interest is supposed to be associated with some observable outcome, then a proxy for that variable should be a good predictor of the outcome.
By prediction we mean out-of-sample prediction, with data not used to construct the proxy itself.
Happily, contemporary machine learning tools make it easy to construct proxy variables according to this criterion, as long as data on the relevant outcomes are available.
The bulk of the article consists of an illustration of our approach, an application to the measurement of relative military power between countries.
Though power is as important to international relations as legislator ideology is to American politics and regime type is to comparative politics, there has been little innovation in measuring military power.\footnote{%
  A recent exception is \citet{Arena:2012}.
}
We show that the standard measure, the ratio of Composite Index of National Capability scores \citep{singer1972}, is a disaster by our predictive criterion.
The capability ratio is only 1~percent better than a null model, or random guessing, at predicting militarized dispute outcomes.
We use ensemble learning to develop a new proxy, the Dispute Outcome Expectations score, that provides a 20~percent predictive improvement.

Like Ulysses or Goldilocks, the creator of a new proxy variable must maintain a delicate balance.
She must learn from historical data to construct the measure, lest it fail to capture important dimensions of the concept under study.
\emph{A priori} measures like summed rating scales suffer from this \emph{underfitting} problem---they fail to take advantage of the wealth of data scholars now possess.
But the scholar who uses a data model to construct a proxy faces pitfalls too.
She runs the risk of mistaking chance features of her data to be systematic ones, or \emph{overfitting}.
A good proxy should fit the data well, but not so well that it fails to generalize.
The criterion we advocate, out-of-sample prediction, balances these two considerations.
A proxy that learns too little will of course be a poor predictor, but so will a data-driven proxy that maximizes in-sample fit at the expense of generalizability.

Supervised learning techniques, having been designed to navigate the straits between underfitting and overfitting, are ideal for data-driven proxy construction.
Machine learning models are flexible enough to model relationships far more complex than possible in ordinary regression or measurement models.
But they also guard against connecting the dots too aggressively, or misinterpreting noise in the data as a complex relationship.
Virtually every supervised learning method has a set of tuning parameters that govern how much flexibility to allow for---in effect, to what extent to treat variation in the data as signal rather than noise.
To develop a model that is optimal for out-of-sample prediction, all an analyst needs to do is choose appropriate tuning parameters, usually by a method like cross-validation that estimates prediction error \citep{Efron:2012es}.
Our approach mirrors that of \citet{Hill:2014ki}, who use cross-validation to assess the relative predictive power of many variables all thought to affect the same outcome.
Our focus, however, is on constructing measures rather than comparing them---in particular, we examine how to create proxies with the greatest ability to predict.

The end result of our analysis is the Dispute Outcome Expectation score, or DOE score.
We use the data on the outcomes of militarized interstate disputes \citep{Palmer:2015hp} to model the relationship between material capability holdings and dispute outcomes, all while optimizing for predictive power.
For every dyad-year from 1816 to 2007, we use this model to estimate the probability that each state would win a hypothetical dispute (and the probability it would end in a stalemate).
DOE scores therefore have the same temporal and spatial coverage as the current state of the art, the capability ratio, but with two additional advantages.
First, in the cases where disputes did occur, the DOE scores are much better predictors of the outcome than the capability ratio.
Second, the DOE score is directly interpretable as the probability of victory, an important concept in the literature on bargaining and war \citep{fearon1995,powell1996stability}.

When we construct a new proxy by optimizing over predictive power, it is almost tautological that it will predict the given outcome better than the existing alternatives.
For a more even-handed comparison, we can take a sample of typical applications of the old proxy and see whether the new one accomplishes them even better.
For example, international relations scholars include the capability ratio, the standard proxy for relative military power, in models of dependent variables other than dispute outcomes (most commonly, whether a dispute takes place at all).
We reanalyze 18 such empirical models to see whether they fit better when we replace the standard proxy with DOE scores.
Since these studies examine outcomes besides the one we use to construct our proxy---namely, victory or loss in international disputes---there is no guarantee that our new proxy will do better.
Nonetheless, we yield an improvement in fit in at least 14 of the 18 cases.
We encourage the creators of future proxies, both in this domain and others, to conduct similarly systematic and comparative studies of their variables' performance in typical use cases.

Although our main goal is to develop a better proxy for military power rather than to test hypotheses about the determinants of power, we do gain some broad substantive insights from the model-building process.
One is that material capabilities indeed matter for military power, as we can explain a substantial amount of the variation in militarized dispute outcomes just with variables on material capabilities.
This finding runs in contrast to the classic study by \citet{Maoz:1983cw}, who finds no relationship between mat√©riel and militarized dispute outcomes.
Our results suggest that this finding is the artifact of relying on ratios of capability holdings, which are a poor proxy for relative material power.
Another important conclusion from our results is that the martial effectiveness of the various material capability components varies over time, which the standard measure does not allow for.

The paper proceeds in five sections.
In the first, we lay out our general argument about proxy construction and its application to the case of military power.
Section~\ref{sec:methods} describes the data and methods we use to construct a new proxy for expected dispute outcomes.
In Section~\ref{sec:scores}, we discuss the advantages and disadvantages of our new measure, the DOE score.
Section~\ref{sec:replications} provides the results of our replications.
The final section addresses next steps and concludes.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "doe"
%%% End:
