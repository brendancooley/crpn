\section{Introduction}

Of all the challenges in political science, perhaps none is more difficult and rewarding than measuring theoretical quantities. 
Sometimes it seems like our most important concepts are the most elusive to measure. 
Methodologists have mitigated old frustrations by developing better models for measuring a variety of quantities, from ideal points \citep{clinton2004} to judicial independence \citep{linzer2014} to democracy \citep{jackman2008}. 
At the same time, the discipline has amassed an impressive amount of data, particularly historical data. 
Ideal points now use roll calls back to the American Constitutional Convention \citep{heckelman2013}; conflict scholars can access industrial output figures for each state dating back to the Napoleonic Wars \citep{singer1972}. 
Improvements in computing power, coupled with scholarly ingenuity, ensure that this progress will continue for the foreseeable future.

We should feel sanguine given these advances, but we should also pause to consider the nature of the measurement enterprise. 
As our models identify more parameters, so too do we face more risk of overfitting: the attribution of systematic importance to random error. 
Likewise, as our data spans more rows, so too do we run the risk of attributing too much reliability to our potentially overfit results. 
Yet, to our knowledge, none of the recent advances in political measurement have taken out-of-sample performance into account; rather, attention is paid to developing and interpreting measures that best reflect extant data.\footnote{In contrast, structural modelers have paid increasing attention to overfitting problems \citep{pitt2002,preacher2006}, though most instruction retains its focus on fit.} 
This is especially unfortunate given that many abstract quantities, particularly those based on formal models of decision under uncertainty, reflect expectations. 
However, the measurement of any quantity suffers when a data set's unique peculiarities are assigned too much explanatory import.

Many new measurement models are explicitly built on theoretical foundations. 
To make the point more concrete, consider ideal point models, which link directly to formal models of spatial voting in the unidimensional setting. 
The empirical variants often impose certain functional forms on relevant quantities; for example, in the pathbreaking model of \citet{poole1985}, the probability that a legislator votes yea on a given issue is explicitly pinned down as a ratio of distances between ideal points and the spatial location of yea and nay alternatives. 
Given the deep relationship between the theoretical and statistical models, and given that empirical scholars cannot avoid the fact that ``all measurement is theory testing'' \citep[272]{jacoby1999}, such functional impositions should be a subject of empirical concern in developing good proxies of theoretical quantities. 
We often ignore functional assessment due to computational issues or because statistical identification rests precipitously on these impositions \citep{poole1991}.

In this article, we argue that proxies should constructed to predict well and that functional forms should be assessed on that criterion. 
We advocate a data-drive approach inspired by the train-validate-test paradigm from the machine learning literature. 
Within the training set, a battery of models is evaluated, and a super learning algorithm assigns weights to the battery to maximize predictive performance. 
The resulting proxy suffers neither from overfitting nor excess reliance on a particular model.

We apply our approach to political power, which arises in all areas of political science but is especially important in the study of international conflict. 
The bargaining model of war \citep{fearon1995}---long the workhorse model in modern IR theory---operationalizes power into expected dispute outcomes, most often represented by the probability that one state defeats another in battle, denoted $p$. 
Given the bargaining model's importance, it comes as no surprise that empirical scholars have sought to include a proxy for $p$ in their analyses. 
Looking at recent (2006--2014) issues of journals that commonly publish papers in this area (\emph{American Political Science Review}, \emph{American Journal of Political Science}, \emph{Journal of Politics}, \emph{International Organization}, and \emph{International Studies Quarterly}) 94 articles include some proxy for $p$ in regression analyses at the dyadic or directed-dyadic level.

The vast majority of these utilize the relevant states' Composite Indices of National Capabilities, or CINC scores \citep{singer1972}. CINC ratios are inappropriate as a proxy for expected dispute outcomes for a variety of reasons, including problems with the CINC function itself, \emph{ad hoc} parameterizations, and issues of functional form. 
What is more, when evaluated on predictive performance, we find that \emph{capability ratios predict nothing}: they fail to predict any outcome other than the modal Stalemate and barely improve out-of-sample predictive performance (1.2\%) over a null model. 
Conversely, our measure---the DOE (Dispute Outcome Expectations) score---improves out-of-sample predictive performance by 16.8\%. 
It is notable that this is the case despite the fact that we include no new covariates over the CINC ratio.

The analysis proceeds in four sections. 
In the first two, we argue for the importance of predictive power in constructing proxies and assess the functional problems associated with capability ratios. 
Section three is the essay's core, with a description of our data, method, and results. 
We pay particular attention to the interpretation of our new measure. 
The final section addresses next steps and concludes. 
