%!TEX root = doe.tex

\section{The New Measure: Dispute Outcome Expectations}
\label{sec:scores}

We use the super learner results to construct a new proxy for expected dispute outcomes---one that predicts actual dispute outcomes much more accurately than the capability ratio does.
For any pair of countries at a particular point in time, whether or not they actually had a dispute with each other, we can use the super learner to ask, ``Based on what we know about their material capabilities, how would a dispute between these countries be likely to end?''
To construct the new proxy, we use the super learner to make predictions for every directed dyad--year in the international system between 1816 and 2007, the range of years covered by the National Material Capabilities data.
We call the resulting dataset the Dispute Outcome Expectations data, or DOE.
The DOE data contains predictions for more than 1.5~million directed dyad--years.\footnote{%
  About 19~percent of directed dyad--years contain missing values of at least one capability component.
  We average across imputations of the capabilities data to calculate the DOE scores for these cases.
  See the Appendix for details.
}
The canonical correlation between the DOE scores and the capability ratio is 0.44, so the measures are related but distinct.

The DOE scores are extrapolations.
The overwhelming majority of dyad-years do not experience a dispute, and those that do are of course systematically different from those that do not.
While we see the DOE scores as a significant advance in the state of the art of measuring power, we advise caution in their interpretation, particularly for dyads that would be unlikely to find themselves in a dispute.
As the output of a model, DOE scores are estimates, and as such they may be subject to selection bias.
A promising direction for future work would be to develop data-driven proxies for power that preserve the flexibility of the super learner while more explicitly correcting for selection bias.

The DOE scores are naturally directed, since each dispute in our training data contains an initiating side and a target side.
However, many analyses in the international conflict literature (e.g., of dispute occurrence) use undirected data.
We calculate undirected DOE scores through a simple average of the directed values.
For example, to calculate the probability that the United States would win a dispute against the United Kingdom in 1816, we average its estimated chances of victory as an initiator (36~percent) and as a target (11~percent) to yield 23.5~percent.
If an analyst using the DOE data believed that the likely identity of an initiator in a hypothetical dispute were not a coin flip, she could take a different average of the directed scores to produce a more representative undirected score.

Perhaps counterintuitively, DOE scores should not be included as controls in regressions whose dependent variable is the outcome of a dispute or war.
This may seem contradictory, given how much effort we have just spent showing that DOE scores are superior predictors of dispute outcomes.
The reason they are superior is that, unlike the capability ratio, they are calibrated using real dispute data.
But this in turn means that DOE scores would be endogenous in a regression whose dependent variable is dispute outcomes---i.e., the same data we used to construct the DOE scores.

In light of the DOE scores' superior predictive performance in the Militarized Interstate Disputes data, we are inclined to believe they dominate the capability ratio as a proxy for expected dispute outcomes.
Next, we test this conjecture by seeing if replacing the capability ratio with DOE scores in empirical models of international conflict improves their in-sample fit and out-of-sample predictive power.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "doe"
%%% End:
