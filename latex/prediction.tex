\section{Predictive Power and Proxy Variables}

We are often interested in questions that link observed data to some unobserved quantity. 
This latter quantity may be unobserved because it is difficult (or impossible) to measure directly (like wealth) or because it is an abstraction (like the ideal point of a voter in a spatial model). 
In either case, the applied analyst faces a choice between omitting some potentially important variable and including some proxy variable in its stead \citep{stahlecker1993}. 
There is no best choice: some theoretical econometricians \citep[e.g.][]{mccallum1972} argue for the inclusion of all proxies (including crude ones), while others \citep[e.g.][]{maddala1977} support only the use of reliable proxies. 
Even those in the former camp, however, admit that reliable proxies perform better than unreliable ones.

Healthy disciplines utilize good measures for central concepts \citep{kuhn1977}, and so social science progresses, in part, by developing better ways to construct proxy variables.\footnote{Here we focus on the importance of models in producing measures; equal weight should be assigned to advances in the estimation of these models' relevant parameters, most notably to advances in Bayesian estimation \citep{jackman2001,martin2002,clinton2004,bafumi2005}.} 
Much recent progress is due to the development of measurement models.\footnote{Of course, the use of theory in the act of measurement is nothing new. 
  Economics retains its longstanding commitment to structural estimation whereby theory is used to uncover theoretically relevant quantities. 
  For current applications to the structural estimation of dynamic discrete-choice games (for example), see \citet{su2012} and \citet{egesdal2013}.} \citet[2]{jacoby2014} observes:
\begin{quote}
  ``All of us are comfortable with the notion of statistical \emph{models} that provide representations of structural relationships between variables.  But, modern social science also regards measurement as a model that pertains to each of the individual variables.  Careful attention and rigorous approaches are just as important for the latter type of models, as they are for the former.''
\end{quote}
Moreover, when the unobserved quantity is an abstraction, appropriate measurement models allow the analyst to perform direct tests that follow from the same set of assumptions as those used in the original, theoretical model. 
As \citet[355]{clinton2004} put it in the context of testing legislative behavior, ``it is inappropriate to use ideal points estimated under one set of assumptions (such as sincere voting over a unidimensional policy space) to test a different behavioral model (such as log-rolling).''

While better models (and better ways to estimate their parameters) have improved our measures of a variety of important quantities, it remains problematic that modern political measurement has ignored the importance of predictive power in producing proxies. 
This is odd, as seminal contributions to the literature utilize classification as a way to prove a new measure's superiority over extant ones. For example, in the classic paper on ideal point estimation in American legislatures, \citet[Table 3]{poole1985} report that a simple classification approach based on their NOMINATE scores correctly predicts over 80\% of legislative votes in most years in their data. Though their procedure estimates ideal points via the method of maximum likelihood rather than via a classification criterion, it remains that this analysis lies prone to textbook overfitting problems. 
In their analysis, Poole and Rosenthal estimate ideal points within a single Congressional session, and their classification analysis then uses those ideal points to assess voting within the same Congressional session. 
While many correct classifications reflect the spatial model's explanatory virtues, others may arise due to overfitting to the data within that Congressional session.

% TODO: This next paragraph is really good, and might merit being moved to the introduction, particularly as part of making clearer exactly what we're doing.

While ideal points may be of interest for purely explanatory purposes \citep{clarke2012}, many other quantities of interest are inherently forward-looking. 
For example, in the bargaining model of war \citep{fearon1995}, states' security decisions are largely a function of their expectations of winning a war.\footnote{Costs, too, play an important role, but their measurement is far more nuanced \citep{stiglitz2008,stiglitz2012}.} 
To animate the situation, imagine a real-world leader that must decide whether to start a war against another state. 
Perhaps inspired by Santayana's observation that ``those who cannot remember the past are condemned to repeat it,'' the leader orders her statisticians to obtain data on the outcomes of previous conflicts and the material capabilities of their combatants. 
The statisticians, of course, could use the data in a variety of ways to produce a prediction for the hypothetical war in question, but the leader would care only that the prediction was the one that did the best job of predicting. 
More to the point, the leader would care less that the relevant parameters fit the historical data as well as possible (as would be the case if the statisticians ran traditional logit or probit models alone) and would care more that the prediction was of high quality. 
Indeed, to borrow another aphorism, we can reimagine the oft-lamented sin of ``fighting the last war'' \citep[e.g.][]{hart1972} as overfitting such historical models with excess weight placed on recent observations. 
Just like the leader, users of the bargaining model want the estimate of war outcomes that predicts best rather than the one that fights the last war.

It is for these reasons that we advocate the train-validate-test approach common in machine learning. 
While orthodox approaches conform to maximization or minimization of relevant error or likelihood within the entire data set, we instead aim to optimize predictive performance.\footnote{Our formal criterion for predictive performance, the log loss function, is explicitly described in the methods section.} 
First, we split our data into a larger \emph{training set} used for model fitting and selection and a smaller \emph{test set} that we use to assess the predictive performance of our chosen model.
We then fit a variety of models to the training set in order to find the one with the best predictive performance.
At this point, if we were to pick the model that best fits the training data, we would likely end up with one that is overfit.
To prevent this, we assess model fit through cross-validation, which amounts to another layer of sample-splitting.
Finally, once we have chosen a model from the training set, we apply it to the test set to yield an accurate measure of its true predictive power when brought to previously unseen data.

This approach explicitly addresses the problems enumerated above: it avoids the overfitting problems associated with traditional measurement techniques and the model selection problems associated with attempts to bring new data to bear for predictive purposes. 
We believe that the costs of our approach---additional computation, interpretative nuance, and conservatism in inference---pale in comparison to these benefits. 

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "doe"
%%% End:
