\section{Predictive Power and Proxy Variables}
We are often interested in questions that link observed data to some unobserved quantity.  This latter quantity may be unobserved because it is difficult (or impossible) to measure directly (like wealth) or because it is an abstraction (like the ideal point of a voter in a spatial model).  In either case, the applied analyst faces a choice between omitting some potentially important variable and including some proxy variable in its stead \citep{stahlecker1993}.  There is no best choice: some theoretical econometricians \citep[e.g.][]{mccallum1972} argue for the inclusion of all proxies (including crude ones), while others \citep[e.g.][]{maddala1977} support only the use of reliable proxies.  Even those in the former camp, however, admit that reliable proxies perform better than unreliable ones.

Healthy disciplines utilize good measures for central concepts \citep{kuhn1977}, and so social science progresses, in part, by developing better ways to construct proxy variables.\footnote{Here we focus on the importance of models in producing measures; equal weight should be assigned to advances in the estimation of these models' relevant parameters, most notably to advances in Bayesian estimation \citep{jackman2001,martin2002,clinton2004,bafumi2005}.}  Much recent progress is due to the development of measurement models.\footnote{Of course, the use of theory in the act of measurement is nothing new.  Economics retains its longstanding commitment to structural estimation whereby theory is used to uncover theoretically relevant quantities.  For current applications to the structural estimation of dynamic discrete-choice games (for example), see \citet{su2012} and \citet{egesdal2013}.} \citet[2]{jacoby2014} observes:
\begin{quote}
  ``All of us are comfortable with the notion of statistical \emph{models} that provide representations of structural relationships between variables.  But, modern social science also regards measurement as a model that pertains to each of the individual variables.  Careful attention and rigorous approaches are just as important for the latter type of models, as they are for the former.''
\end{quote}
Moreover, when the unobserved quantity is an abstraction, appropriate measurement models allow the analyst to perform direct tests that follow from the same set of assumptions as those used in the original, theoretical model.  As \citet[355]{clinton2004} put it in the context of testing legislative behavior, ``it is inappropriate to use ideal points estimated under one set of assumptions (such as sincere voting over a unidimensional policy space) to test a different behavioral model (such as log-rolling).''

While better models (and better ways to estimate their parameters) have improved our measures of a variety of important quantities, it remains problematic that modern political measurement has ignored the importance of predictive power in producing proxies.  This is odd, as seminal contributions to the literature utilize classification as a way to prove a new measure's superiority over extant ones. For example, in the classic paper on ideal point estimation in American legislatures, \citet[Table 3]{poole1985} report that a simple classification approach based on their NOMINATE scores correctly predicts over 80\% of legislative votes in most years in their data. Though their procedure estimates ideal points via the method of maximum likelihood rather than via a classification criterion, it remains that this analysis lies prone to textbook overfitting problems.  In their analysis, Poole and Rosenthal estimate ideal points within a single Congressional session, and their classification analysis then uses those ideal points to assess voting within the same Congressional session.  While many correct classifications reflect the spatial model's explanatory virtues, others may arise due to overfitting to the data within that Congressional session.

What is more, ideal points from one Congressional session cannot tell us about voting behavior in other Congressional sessions absent strong theoretical impositions.  Even with the use of dynamic models for ideal point estimation \citep{martin2002}, measurement models alone cannot predict to situations outside the sample.  The most obvious workaround is the inclusion of covariates in the estimation of ideal points;\footnote{An easier, but less appropriate, approach would be to estimate some second model using the measure on the left hand side and then use the results from that model for prediction using future covariates.} more modern estimation approaches \citep{clinton2004} allow for the inclusion of covariates, but their use is not widespread \citep[for an exception, see][]{bailey2001}.  Even with covariates included, however, the estimates obtained from the complete data remain overfit (per the paragraph above), and so this remedy provides no panacea to the prediction problem.  Further, typical applications including covariates fail to employ appropriate methods to examine the quality of a given specification.  Subsequently, even restricting attention to overfit models, the estimates obtained may or may not represent the data best.

While ideal points may be of interest for purely explanatory purposes \citep{clarke2012}, many other quantities of interest are inherently forward-looking.  For example, in the bargaining model of war \citep{fearon1995}, states' security decisions are largely a function of their expectations of winning a war.\footnote{Costs, too, play an important role, but their measurement is far more nuanced \citep{stiglitz2008,stiglitz2012}.}  To animate the situation, imagine a real-world leader that must decide whether to start a war against another state.  Perhaps inspired by Santayana's observation that ``those who cannot remember the past are condemned to repeat it,'' the leader orders her statisticians to obtain data on the outcomes of previous conflicts and the material capabilities of their combatants.  The statisticians, of course, could use the data in a variety of ways to produce a prediction for the hypothetical war in question, but the leader would care only that the prediction was the one that did the best job of predicting.  More to the point, the leader would care less that the relevant parameters fit the historical data as well as possible (as would be the case if the statisticians ran traditional logit or probit models alone) and would care more that the prediction was of high quality.  Indeed, to borrow another aphorism, we can reimagine the oft-lamented sin of ``fighting the last war'' \citep[e.g.][]{hart1972} as overfitting such historical models with excess weight placed on recent observations.  Just like the leader, users of the bargaining model want the estimate of war outcomes that predicts best rather than the one that fights the last war.

It is for these reasons that we advocate the train-validate-test paradigm from the machine learning literature \note{cite}.  We thereby abandon the goal of uncovering some truth undergirding the data and instead focus on creating measures that predict as well as possible.\footnote{Our formal criterion for predictive performance, the log-loss function, is explicitly described in the methods section.}  While orthodox approaches conform to maximization or minimization of relevant error or likelihood within the entire data set, we instead aim to optimize predictive performance.  In classic machine learning examples, the complete data set is partitioned into a larger \emph{training set} and a smaller \emph{test set} via random draw.  The randomness of this assignment ensures that the distribution underneath the two subsets is the same.  Some algorithm is performed only on the training set in the hopes that the results predict well in the test set.  However, since predictive performance is our primary criterion, we train, validate, and test \emph{within} the training set as well.  More explicitly, we utilize $k$-fold cross-validation within the training set to obtain results (across a variety of models) that maximize predictive performance.  The weighted average of this ensemble of methods produces our final super-model, which in turn is assessed using the original test set.

This approach explicitly addresses the problems enumerated above:  it avoids the overfitting problems associated with traditional measurement techniques and the model selection problems associated with attempts to bring new data to bear for predictive purposes.  We believe that the costs of our approach---additional computation, interpretative nuance, and conservatism in inference---pale in comparison to these benefits. 