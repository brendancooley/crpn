%!TEX root = doe.tex

\section{Building a Better Proxy for Relative Military Power}
\label{sec:methods}

Our goal now is to squeeze as much predictive power as we can from data on states' material capabilities.
When prediction is the goal, ``black box'' algorithmic techniques usually outpace standard regression models \citep{Breiman:2001fd}.
So, to build our new measure, we augment traditional approaches with methods from machine learning.

\subsection{Data}

We combine the National Material Capabilities data \citep{singer1972} with information on the outcomes and participants of Militarized International Disputes between 1816 and 2007 \citep{Palmer:2015hp}.
Our data consist of $N = 1{,}740$ disputes, each between an ``initiator,'' or Country~A, and a ``target,'' or Country~B.\footnote{%
  See the Appendix for the data construction and coding specifics.
}
Every dispute outcome is either A~Wins, B~Wins, or Stalemate, denoted $Y_i \in \{A, B, \emptyset\}$.
Most disputes end in a stalemate, and victory by the initiator is more than twice as likely as victory by the target, as shown in Table~\ref{tab:mid}.

\begin{table}[htp]
  \centering
  \input{tab-mid}
  \caption{
    Distribution of the three dispute outcomes.
  }
  \label{tab:mid}
\end{table}

We model dispute outcomes as a function of the participants' military capabilities.
Our data source, the National Material Capabilities dataset, records annual observations of six characteristics of a country's military capability: military expenditures, military personnel, iron and steel production, primary energy consumption, total population, and urban population.\footnote{%
  There are missing observations in the National Material Capabilities data.
  Consequently, about 17~percent of the disputes we observe contain at least one missing cell.
  We use multiple imputation to deal with missingness \citep{honaker_what_2010}; see the Appendix for details.
}
We also calculate each country's share of the global total of each component, giving us 12 variables per dispute participant.
The matrix of predictors has 26 columns: the 24 individual capability characteristics of the initiator and target, the standard capability ratio, and the year the dispute began.
Collect these predictors for the $i^{\text{th}}$ dispute into the vector $X_i$.

\subsection{A Metric for Predictive Power}

As fortune plays a role in every military engagement, it is impossible to perfectly predict the outcome of every dispute.
We therefore want a measure of predictive power that respects the probabilistic nature of militarized disputes.
Classification metrics like the accuracy statistic, also known as the percentage correctly predicted, do not fit the bill.\footnote{%
   Such classification metrics also discriminate poorly with imbalanced classes like ours \citep[420--423]{kuhn}.
}
Instead, we employ the log loss, which is the negative of the average log-likelihood, as our metric for predictive power \citep[221]{Hastie:2009wpa}.
Let a \emph{model} be a function $\hat{f}$ that maps from the dispute-level predictors $X_i$ into the probability of each potential dispute outcome, $\hat{f}(X_i) = (\hat{f}_A(X_i), \hat{f}_B(X_i), \hat{f}_{\emptyset}(X_i))$.
The ``hat'' on $\hat{f}$ emphasizes that the form of the function has been learned from the data, whether by estimating regression coefficients or by a more flexible predictive algorithm.
The log loss of model~$\hat{f}$ on the data~$(X, Y)$ is\footnote{%
  To avoid numerical problems, very low probabilities are trimmed at $\epsilon = 10^{-14}$.
}
\begin{equation}
  \label{eq:log-loss}
  \ell(\hat{f}, X, Y)
  =
  - \frac{1}{N} \sum_{i = 1}^{N} \sum_{y \in \{A, B, \emptyset\}}
  \mathbf{1} \{Y_i = y\} \log \hat{f}_y(X_i).
\end{equation}
Smaller values of the log loss represent better predictive power, with the lower bound of~$0$ indicating perfect prediction.

We care mainly about the generalization error of our models---the expected quality of their predictions for new data that was not used to fit the models.
To measure out-of-sample predictive power without losing data, we use $K$-fold cross-validation \citep[241--249]{Hastie:2009wpa}.\footnote{%
  \label{fn:nested-cv}%
  When dealing with models with tuning parameters that are themselves selected by cross-validation, we choose tuning parameters separately within each of the $K$ iterations via another cross-validation loop \citep{Varma:2006ch}.
}
Following standard practice, we set $K = 10$.
Let $\CVL(\hat{f})$ denote the 10-fold cross-validation estimate of the out-of-sample log loss.
To ease interpretation, we compare models' log loss to that of a null model, whose predicted probabilities always equal the sample proportions of each outcome.
The proportional reduction in cross-validation loss of the model~$\hat{f}$ is
\begin{equation}
  \label{eq:prl}
  \PRL(\hat{f})
  =
  \frac{
    \CVL(\hat{f}_{\text{null}}) - \CVL(\hat{f})
  }{
    \CVL(\hat{f}_{\text{null}})
  }.
\end{equation}
The theoretical maximum, for a model that predicts perfectly, is~$1$.
If a model predicts even worse than the null model---meaning it is worse than random guessing---its proportional reduction in loss is negative.

\subsection{Modeling Dispute Outcomes}

Our task now is twofold: to assess the predictive power of the capability ratio and, should we find it lacking (as we do), to build a better alternative.

We model dispute outcomes as a function of the capability ratio via ordered logistic regression \citep{McKelvey:2010gv}.
To reduce skewness, we take the natural logarithm of the capability ratio.
The parameter estimates from the capability ratio model on the full sample appear in Table~\ref{tab:capratio}.
Although these results do not speak directly to the capability ratio's out-of-sample performance, they foreshadow why its predictive power is so limited.
The coefficient on the capability ratio is statistically significant but small enough relative to the cutpoints that it always predicts a stalemate within the sample.
This does not bode well for its out-of-sample performance.

\begin{table}[tp]
  \centering
  \input{tab-capratio}
  \caption{
    Results of an ordered logistic regression of dispute outcomes on the capability ratio using the training data.
  }
  \label{tab:capratio}
\end{table}

We want a better model than what the capability ratio gives us, but we do not have a strong \emph{a priori} sense of what the true relationship between material capabilities and dispute outcomes looks like.
We use tools from machine learning that are designed to predict well without imposing much structure on the data, drawing a set of candidate models from the top-ten list by \citet{Wu:2007ev} and from the best performers in the tests by \citet{FernandezDelgado:2014ul}.
After excluding those unsuited to our data, we end up with six predictive algorithms: C5.0, support vector machines, $k$-nearest neighbors, classification and regression trees, random forests, and ensembles of neural nets.\footnote{%
  See the Appendix for full details of each method.
}
Each algorithm is widely used for prediction and can predict dispute outcome probabilities as a complex, potentially nonlinear function of the material capability components.
As a compromise between these flexible ``black box'' models and the rigid capability ratio model, we also test ordered logistic regression models on the capability components.

In the spirit of flexibility, we try each model with different sets of predictors from the capability data.
We examine four sets of variables: the raw capability components and the annual component shares, each with and without the year the dispute began.
All of our models allow for interactive relationships, so including the year of
the dispute lets the effect of each capability component vary over time.
With two sides per dispute and six capability variables per side, each model has 12 or 13 variables, depending on whether the year is included.
To ensure that the models with the year included are not just picking up differences in the distribution of outcomes over time (see Figure~\ref{fig:outcomes-time}), we also include an ordered logit of outcome on a third-order polynomial for year \citep{Carter:2010dh}, a post-1945 dummy, and their interaction.
All told, we have 31 candidate models: four sets of variables for each of our seven algorithms, plus the capability ratio model, the time trend model, and a null model used as a baseline.

\begin{figure}[tp]
  \centering
  \input{fig-outcomes-time}
  \caption{
    Distribution of dispute outcomes over time.
    Values are the proportion of disputes in the prior ten years ending in the given outcome.
  }
  \label{fig:outcomes-time}
\end{figure}

We use cross-validation to estimate how well each of our candidate models predicts out of sample.
The final problem, given those estimates, is to choose a model to construct an alternative to the capability ratio as a measure of expected dispute outcomes.
It is tempting to simply pick the model with the lowest cross-validation loss.
We can do even better at prediction, however, by taking a weighted average of all the models.
We use the super learner algorithm \citep{vanderLaan:bz} to select the optimal model weights via cross-validation.
Given a set of $M$ candidate models $\hat{f}_1, \ldots, \hat{f}_M$, we select weights $\hat{w}_1, \ldots, \hat{w}_M$ to solve the constrained optimization problem
\begin{equation}
  \label{eq:super-learner}
  \begin{aligned}
    \min_{w_1, \ldots, w_M}
    &\quad
    \CVL \left(
      \sum_{m=1}^M w_m \hat{f}_m
    \right)
    \\
    \mbox{s.t.}
    &\quad
    w_1, \ldots, w_m \geq 0,
    \\
    &\quad
    w_1 + \ldots + w_m = 1,
  \end{aligned}
\end{equation}
Our final model is the super learner, $\hat{f} = \sum_m \hat{w}_m \hat{f}_m$.
Each individual model is a special case of the super learner, with full weight $\hat{w}_m = 1$ placed on a single $\hat{f}_m$.
Hence, by the cross-validation criterion, we should prefer the super learner over any individual model.\footnote{%
  \label{fn:sl-bias}
  As usual when selecting tuning parameters via cross-validation, the value of equation~\eqref{eq:super-learner} is not an unbiased estimate of the generalization error of the super learner.
  Nested cross-validation is computationally infeasible for the super learner, so we calculate the bias correction recommended by \citet{Tibshirani:2009tz} to estimate its generalization error.
}

\subsection{Cross-Validation Results}

We now turn to the cross-validation results, which are summarized along with the super learner weights in Table~\ref{tab:ensemble}.
As the in-sample analysis hinted, the capability ratio is indeed a poor predictor of dispute outcomes.
Its proportional reduction in loss is~0.01, which means its predicted probabilities are just 1~percent more accurate than the null model.
This number is not encouraging, but what matters even more is whether we can do better.
A glance at Table~\ref{tab:ensemble} confirms that we can: all but one of our 28 alternative models have greater predictive power than the capability ratio, many of them considerably better.
With these results in hand, we feel comfortable dismissing the capability ratio as a suboptimal proxy for expected dispute outcomes.

\begin{table}[tp]
  \centering
  \input{tab-ensemble}
  \caption{
    Summary of cross-validation results and super learner weights.
    All quantities represent the average across imputed datasets.
  }
  \label{tab:ensemble}
\end{table}

As we expected, the super learner ensemble performs better than any of the candidate models from which it is constructed.
The ensemble's proportional reduction in loss is about 23~percent, or four percentage points better than the best candidate model.
Even after we apply a bias correction (see footnotes~\ref{fn:nested-cv} and~\ref{fn:sl-bias}), the super learner's predictive power is still the best among our models.
Looking at the weights, what stands out is how few models are substantial components of the super learner: just five models have a weight of at least 5~percent.
More generally, while models with lower generalization error tend to receive more weight, the relationship is by no means one-to-one.
We see this because the ensemble prefers not only predictive power, but also diversity.
Different classes of models have different blind spots; the more diverse the ensemble is, the more these blind spots are minimized.
A model that looks bad on its own might still merit non-negligible weight in the optimal ensemble if it captures a slice of the data missed by the models that are best on their own.

\begin{figure}[tp]
  \centering
  \input{fig-oof-pred}
  \vspace{-2em}
  \caption{
    Ternary plots of out-of-fold predicted probabilities according to the capability ratio model and the super learner.
    Each predicted probability is calculated by fitting the model to 9/10 of the data, not including the observation in question---an approach that simulates true out-of-sample prediction.
  }
  \label{fig:oof-pred}
\end{figure}

For another illustration of the difference in predictive power between our model and the capability ratio, see the plots of out-of-fold predicted probabilities---the ones we use in cross-validation---in Figure~\ref{fig:oof-pred}.
Under the capability ratio model, all but a handful of disputes are predicted to have an 80--90~percent chance of ending in stalemate.
Seeing how narrow the capability ratio's predictive range is, it is little surprise that it barely does better than a null model at prediction.
Conversely, the super learner makes much better use of the material capability data.
Its predictive range is greater, which in turn allows it to achieve a stronger, though hardly perfect, relationship between predicted and observed outcomes.

\subsection{Implications for International Relations}

Our main focus is on developing a proxy for relative power that predicts the outcomes of militarized disputes, and predictive approaches like ours are not optimal for testing specific hypotheses \citep{Shmueli:2010jd}.
Nonetheless, we can glean from our results a few important insights about the nature of the relationship between capabilities and power.
The first is that there \emph{is} a relationship---that variation in dispute outcomes is associated with variation in the disputants' raw capabilities.
Our results therefore support the strand of literature finding that material capabilities influence dispute outcomes \citep{wartrap,Stam:1996wl,Sullivan:2012vi}.
Previous findings to the contrary \citep[e.g.,][]{Cannizzo:1980vz,Maoz:1983cw} may simply reflect the inadequacy of CINC-based measures as a proxy for power.

But material power is not all that matters.
Even after an intense, diverse predictive effort, we explain only 20 percent of the variation in dispute outcomes with material capability variables.
To some extent this reflects the inherent unpredictability of military affairs; we would never expect to predict outcomes perfectly.
Another potential source of error is that, depending on the extent of their aims, states may not fully deploy the capabilities they possess \citep{sullivan2007war}.
We suspect, however, that we could predict dispute outcomes even better by conditioning on more observable indicators.
That is a task for future work, as the purpose of this paper is only to develop a proxy for the material components of relative power.

\begin{table}[tp]
  \centering
  \input{tab-varimp}
  \caption{%
    Percentage increases in loss, relative to the full ensemble, due to removing each capability component from the analysis.
    The results ``without year'' come from running the super learner on only the 16 component models without the year variable.
  }
  \label{tab:varimp}
\end{table}

To gauge the relative importance of the material component variables, we rerun the ensemble training six times, each time removing one of the component variables from the analysis.
We then calculate the log loss of the resulting ensemble relative to the original model; greater values indicate more loss of predictive power and thus a more important predictor.
Table~\ref{tab:varimp} contains the results of the variable importance analysis.
Perhaps because the components are correlated with each other, the removal of any single component does not change the results much.
The greatest loss in predictive power comes from dropping primary energy consumption (PEC), an indicator of economic development and industrial capacity.
It is interesting that PEC, a rough catch-all that correlates handily with any number of relevant constructs, winds up playing a stronger direct role than more explicitly militaristic factors like troops or military expenditures.
However, recent work \citep[e.g.][]{beckley2010economic} suggests that economic development is a primary determinant of military effectiveness, while softer contextual factors like regime type, culture, or human capital play a far less vital role.
The $p$ we estimate here is entirely agnostic to functional form, but it seems that our flexible approach has allowed both direct-effort indicators like personnel to matter while also capturing the subtle interactive effects of a military effectiveness variable like economic development.

A second important finding is that the determinants of material power change over time.
This conclusion may sound obvious, but it raises the question of why international relations scholars continue to use a proxy for power that assumes the relationship is unchanging.
The simplest way to observe that time matters is to compare the predictive power of the models with and without the year variable: in 13 out of 14 cases, the model that includes time predicts better than its closest time-less counterpart.\footnote{%
  % UPDATED
  The difference in log loss is statistically significant (paired $t = -3.1$, $p = 0.008$).
}
The ensemble is not simply picking up the changing distribution of dispute outcomes over time: the model with a time trend alone has a mediocre PRL of 0.08, and it receives negligible weight.

For a look at how much the effect of each component changes over time, we turn back to the variable importance analysis.
For each set of models dropping a particular component, we form a new ensemble just using those models without the year variable and compare its log loss to the original ensemble.
In all but one case, the predictive loss due to dropping a capability component and the year of the dispute is more than the sum of the loss due to dropping each alone.
The time variation is most pronounced for military expenditures.
That is, the returns to a dollar spent for military purposes have varied more over time than the returns to a soldier, or on increased industrial capacity, or increased population.
This likely reflects changes in military technology and bureaucracy over time.
As militaries have oscilatted between labor and capital intensity, so too have their requisite expense and the returns on investment \citep{howardWar}.
The same goes for innovations in military bureaucracy.
Interestingly, the component with the most static effect is primary energy consumption---it is the most important indicator on its own, and its effects vary least over time.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "doe"
%%% End:
