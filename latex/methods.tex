\section{Building a Better Proxy for Expected Dispute Outcomes}
\label{sec:methods}

Our goal now is to squeeze as much predictive power as we can from data on states' material capabilities.
When prediction is the goal, ``black box'' algorithmic techniques usually outpace standard regression models \citep{Breiman:2001fd}.
Therefore, to build our new measure, we augment traditional approaches with methods from machine learning.

\subsection{Data}

To evaluate the predictive performance of both the capability ratio and our alternative model, we use data on the outcomes of international disputes.
We combine the National Material Capabilities data \citep{singer1972} with information on the outcomes and participants of Militarized International Disputes between 1816 and 2007 \citep{Palmer:2015hp}.
Our data consist of $N = 1{,}740$ disputes, each between an ``initiator,'' or Country~A, and a ``target,'' or Country~B.\footnote{
  See the Appendix for the data construction and coding specifics.
}
Every dispute outcome is either A~Wins, B~Wins, or Stalemate, which we denote by $Y_i \in \{A, B, \emptyset\}$, respectively.
In each dispute, we observe the year it took place, each disputant's raw material capability components, and each disputant's share of the system-wide total of each component at the time.\footnote{
  There are missing observations in the National Material Capabilities data.
  Consequently, about 17\% of the disputes we observe contain at least one missing cell.
  We use multiple imputation to deal with missingness \citep{honaker_what_2010}; see the Appendix for details.
}
These predictors are collected in the vector $X_i$.

We want to measure the out-of-sample predictive power of various models, which means we cannot use the full dataset for model fitting.
Following common practice in machine learning, we randomly divide our sample into two parts: a \emph{training sample} for building and selecting models, and a \emph{test sample} for making out-of-sample predictions with a model built on the training sample.
We use an 80-20 split, resulting in a training sample of $N_{\text{train}} = 1{,}391$ disputes and a test sample of $N_{\text{test}} = 349$ disputes.\footnote{
  To avoid complications due to missing data, we draw the test sample exclusively from the subset of complete observations.
}
We exclusively use the training sample for all model tuning, fitting, and selection.
Even if we try to prevent overfitting in the training stage---which we do---we cannot rule out the possibility that the resulting model overfits the training data.
This is where the test sample comes in.
After selecting a model from the training data, we run the test sample through it to evaluate the model's out-of-sample predictive power.
As long as we do not use the test sample to make any further modeling decisions, this gives us an unbiased estimate of our chosen model's out-of-sample performance.

\subsection{Modeling Dispute Outcomes}

Our goal is to build the model that squeezes the most out-of-sample predictive power out of the few predictors available to us.\footnote{
  Having more predictors would, of course, be even better---we just want to see how much better we can do than the capability ratio without any additional data collection.
  Our method for constructing a measure would generalize easily to a setting with additional predictors.
}
To begin with, we must be a bit more specific about what we mean by ``predictive power.''
We define a \emph{model} as a function $f$ that maps from the predictor variables into the probability of each potential dispute outcome, $f(X_i) = (f_A(X_i), f_B(X_i), f_{\emptyset}(X_i))$.
Our metric for a model's predictive power is its \emph{log loss}, which is common in multinomial classification settings and is closely related to the log-likelihood \citep[221]{Hastie:2009wpa}.
To calculate the log loss, we take a model's predicted probability of each observed outcome, take the average of their logged values, and multiply the resulting average by $-1$.
Smaller values of the log loss represent greater predictive accuracy, with a lower bound of~$0$ representing perfect prediction.\footnote{
  Predicted probabilities very close to~0 are trimmed to keep the log loss finite.
}
Formally, the log loss of a model~$f$ on the data $(X, Y)$ is
\begin{equation}
  \label{eq:log-loss}
  \ell(f, X, Y)
  =
  - \frac{1}{N} \sum_{i = 1}^{N} \sum_{t \in \{A, B, \emptyset\}}
  \mathbf{1} \{Y_i = t\} \log f_t(X_i).
\end{equation}

To select a model to use for out-of-sample prediction, we will fit a number of candidate models on the training data.
Since we cannot use the test data for model selection, this requires estimating each model's out-of-sample prediction error within the training sample.
We cannot rely on ordinary within-sample measures of fit (e.g., percent correctly predicted), as these will lead us to overfit to the training data.
Instead, we estimate each model's out-of-sample log loss via $K$-fold cross-validation \citep[241--249]{Hastie:2009wpa}.
We randomly assign each observation in the training sample to a ``fold'' $k \in \{1, \ldots, K\}$ and then fit each candidate model $K$ times, each time leaving one fold out of the fitting.\footnote{
  When fitting models with tuning parameters, we choose tuning parameters separately within each fold via another cross-validation loop, again using log loss as the objective function.
  In this case, when choosing the tuning parameters for the model fit to the full training data, we use different folds than those we use to estimate its out-of-fold log loss.
}
Assume that we have $M$ candidate models indexed by $m = 1, \ldots, M$, and let $\hat{f}_m^{(-k)}$ be the result when we fit the $m$'th candidate model to the data excluding fold~$k$.
Our estimate of the out-of-sample log loss of the $m$'th candidate model is its average out-of-fold log loss,
\begin{equation}
  \label{eq:cv-loss}
  \CVL(\hat{f}_m)
  =
  \frac{1}{K} \sum_{k=1}^K \ell \left(
    \hat{f}_m^{(-k)}, X^{(k)}, Y^{(k)}
  \right),
\end{equation}
where $(X^{(k)}, Y^{(k)})$ is the subset of training data assigned to fold~$k$.
Following usual practice, we use $K = 10$ cross-validation folds.

It is well known in the machine learning literature that averaging many models can lead to better predictive accuracy than using a single model \citep{breiman_bagging_1996}.
Accordingly, instead of simply choosing the model with the lowest CV loss, we combine the models in a weighted average following the super learner algorithm \citep{vanderLaan:bz}.
Specifically, we select the weights $\hat{w}_1, \ldots, \hat{w}_M$ that solve
\begin{equation}
  \label{eq:super-learner}
  \begin{aligned}
    \min_{w_1, \ldots, w_M}
    &\quad
    \CVL \left(
      \sum_{m=1}^M w_m \hat{f}_m
    \right)
    \\
    \mbox{s.t.}
    &\quad
    w_1, \ldots, w_m \geq 0,
    \\
    &\quad
    w_1 + \ldots + w_m = 1,
  \end{aligned}
\end{equation}
where each $\hat{f}_m$ is the $m$'th candidate model fit to the full training data.
Our final model is the super learner, $\hat{f} = \sum_m \hat{w}_m \hat{f}_m$.
By definition, the CV loss of the super learner is no greater than that of the best candidate model, which is a special case.

To extract as much predictive power as possible from the material capability data, we examine a diverse array of candidate models to plug into the super learner.
Each candidate model must, of course, work with a categorical outcome variable and generate predicted probabilities.
Beyond that, we restrict our focus to classes of models that have been well studied in the machine learning or statistics literatures, settling on six: ordered logistic regression \citep{McKelvey:2010gv}, $k$-nearest neighbors \citep{Cover:1967jq}, random forests \citep{Breiman:2001fb}, neural networks \citep{Ripley:1996vd}, Gaussian processes \citep{Rasmussen:2006vz}, and support vector machines \citep{Cortes:1995ie}.
We run each model on four sets of variables: the disputants' raw capability components and their capability component proportions, each with and without the year the dispute began.
To mimic the applied literature, we include a standard ordered logistic regression on the capability ratio.
Finally, we include a null model (ordered logistic regression on just an intercept), giving us $M = 26$ candidate models.

The biggest downside of our approach is that the results are not easily interpretable.
Because the super learner entails averaging a large set of models---some of which, like random forests, are themselves difficult to interpret---it gives us no simple summary of how each predictor affects dispute outcomes.
Whether this is a problem depends on one's aims.
Certainly, we would not recommend the super learner as a means of testing hypotheses about the determinants of dispute outcomes.
However, our goal is not to test a hypothesis---it is to construct the best proxy possible for how a dispute between two countries is likely to end.
In this context, it is worth sacrificing interpretability for the sake of predictive power.

\subsection{Results}

The analysis proceeds in two steps.
First, we fit candidate models on the training set, cross-validate them to estimate their out-of-sample prediction error, and average them to form a super learner.
Second, we compare the super learner's predictions to actual outcomes in the test set to obtain an unbiased estimate of its out-of-sample predictive power.
Our main findings are that the capability ratio has barely more predictive power than a null model and that the super learner does much better.
In other words, by making optimal use of the component variables underlying the capability ratio, we can construct a superior proxy for expected dispute outcomes.

\begin{table}[tp]
  \centering
  \input{tab-capratio}
  \caption{
    Results of an ordered logistic regression of dispute outcomes on the capability ratio using the training data.
    Because there are no missing values in the CINC scores, these estimates are identical across imputed datasets.
  }
  \label{tab:capratio}
\end{table}

A glance at the training set results shows that the predictive power of the capability ratio is limited.
We say ``limited'' not as a euphemism for ``bad'' (though it is bad), but rather literally.
To see why, examine Table~\ref{tab:capratio}, which contains the results of the ordered logistic regression on the logged capability ratio in the training data.
Although the coefficient on the capability ratio is positive and statistically significant at the conventional level, its magnitude is small relative to the cutpoints.
In order to predict that A Wins, we would need a logged capability ratio of more than~7---an impossibility, since the logged capability ratio is bounded above by~0.
Similarly, to predict that B Wins, we would need a logged capability ratio of less than~$-13$, which is below the minimal value observed in the training data (about~$-9$).
The predicted probabilities are also constricted: for example, the probability that A Wins is between 2 percent and 14 percent for all observations in the training data.
So the capability ratio only weakly predicts dispute outcomes in sample.
This does not bode well for its ability to make predictions out of sample.

\begin{table}[tp]
  \centering
  \input{tab-ensemble}
  \caption{
    Summary of training set results, including the cross-validation estimate of out-of-sample log loss, proportional reduction in loss (relative to the null model), and optimal super learner weight of each candidate model.
    All quantities represent the average across imputed datasets.
  }
  \label{tab:ensemble}
\end{table}

Indeed, the results are even less encouraging when we turn to the cross-validation results, which give us a formal estimate of the capability ratio's out-of-sample predictive power.
The results for all candidate models, as well as their weights in the super learner ensemble, appear in Table~\ref{tab:ensemble}.
By the criterion of CV loss, the capability ratio does barely better than a null model at predicting dispute outcomes.
Its proportional reduction in loss, a rough analog of $R^2$ for categorical data, is about 1~percent---a negligible improvement.
Consequently, it is no surprise that the capability ratio model receives little weight in the optimal ensemble.\footnote{
  Its average weight across imputations is about $7.5 \times 10^{-7}$.
}
Simply put, the capability ratio's relationship with dispute outcomes is too weak for it to serve as a good proxy for the likelihood that a conflict will end in either side's favor.

On the bright side, when we examine the other candidate models, we see that it is possible to build a better proxy with the same underlying components.
All but two of the other candidate models have better cross-validation loss than the capability ratio model,\footnote{
  The two models that perform worse are the Gaussian processes on the capability proportions.
  We suspect that the strong right skew of the capability proportions throws off the algorithm used for automatic tuning parameter selection in the software we use for Gaussian processes.
  See the Appendix for details.
}
and most have a proportional reduction in loss of 10 percent or greater.
Looking at the performance figures reported in Table~\ref{tab:ensemble}, a few basic patterns emerge.
Models that include the year of the dispute tend to predict outcomes better than those that do not, indicating that the importance of different material capability components varies over time.
We also see, perhaps counterintuitively (and contrary to how the capability ratio is constructed), that the raw capability components tend to be better predictors than the annual totals.
On the modeling front, neural networks tend to perform best, followed by random forests, support vector machines, and $k$-nearest neighbors.
The ordered logistic regression models---significantly less flexible in terms of allowing for nonlinearities and interactions---perform worse.

As we expected, the super learner ensemble performs discernibly better than any of the candidate models from which it is constructed.
The ensemble's proportional reduction in loss, as estimated by cross-validation on the training set, is about 23~percent, or three percentage points better than the best candidate model.
The model concentrates the bulk of its weight on two of the candidate models: the random forest on the raw capability components and the neural network on the annual capability proportions, both with year of dispute included.
Interestingly, though one of these (the neural network) is the best individual model, the other is not the second-best.
More generally, while models with lower CV loss tend to receive more weight, the relationship is by no means one-to-one.
We see this because the ensemble prefers not only predictive power, but also diversity.
Different classes of models have different blind spots; the more diverse the ensemble is, the more these blind spots are minimized.
A model that looks bad on its own might still merit non-negligible weight in the optimal ensemble if it captures a slice of the data missed by the models that are best on their own.

\begin{table}[tp]
  \centering
  \input{tab-test}
  \caption{
    Results of applying the null model, the capability ratio model, and the super learner to the test data.
    P.R.L.\ is the proportional reduction in log loss compared to the null model.
    Accuracy and kappa are measures of classification performance, where we take the predicted outcome to be the one with the highest predicted probability.
    Accuracy is the percentage correctly predicted, while kappa is the percentage improvement in classification over what would be expected by chance \citep{Carletta:1996uk}.
  }
  \label{tab:test}
\end{table}

Armed with these pleasant results, we proceed with the super learner to construct our proxy for expected dispute outcomes.
Since cross-validation estimates of out-of-sample predictive power tend to be too optimistic \citep{Tibshirani:2009tz}, we obtain an unbiased estimate by calculating the super learner's log loss on our test sample.
We reiterate here that the super learner is constructed solely from the training data and that we settled on its use before examining its test set performance.
Had we done otherwise, the test set results might also be too optimistic in expectation.
The results of the test set analysis appear in Table~\ref{tab:test}.
The super learner's out-of-sample proportional reduction in error is about 17~percent---less than our cross-validated estimate, as expected, but still respectable.
Of course, we would like to have a model that does even better at forecasting dispute outcomes out of sample.
We suspect this will take additional data on material capabilities or other factors that affect conflict processes.
Given the diversity and flexibility of the models we apply to the National Material Capabilities data, it is hard to see how any further improvements would be more than marginal.

\begin{figure}[tp]
  \centering
  \input{fig-tern}
  \vspace{-6em}
  \caption{
    Predicted probabilities of dispute outcomes in the test set according to the capability ratio model and the super learner.
    The first row shows predictions for every dispute; the second excludes disputes that ended in a stalemate.
  }
  \label{fig:tern}
\end{figure}

We also use the test data to confirm our pessimism about the capability ratio's out-of-sample predictive power.
We see from Table~\ref{tab:test} that its proportional reduction in loss on the test sample is 1~percent, the same as what we estimated via cross-validation on the training sample.
Beyond that, the test set results nicely illustrate the limited predictive range of the capability ratio model, as shown in the ternary plots in Figure~\ref{fig:tern}.
Under the capability ratio model, plotted in the left column, every dispute in the test set is given an 85--90~percent chance of being a stalemate.
Seeing how narrow the capability ratio's predictive range is, it is little surprise that it barely does better than a null model at prediction.
Conversely, the super learner (depicted in the right column) makes much better use of the material capability data.
Its predictive range is greater, which in turn allows it to achieve a stronger---though hardly perfect---relationship between predicted and observed outcomes.
If our goal is to forecast the expected outcomes of potential international disputes, the super learner gives us a far better proxy than the capability ratio does.

\subsection{The New Measure}

With the super learner results in hand, we construct a new proxy for expected dispute outcomes.
Like all of our candidate models, the trained super learner is a function that maps from the observed capabilities of a pair of states into the probability that a dispute between them will end in the initiator winning, the target winning, or a stalemate.
For any pair of countries at a particular point in time---whether or not they actually had a dispute with each other---we can use the super learner to ask, ``Based on what we know about their material capabilities, how would a dispute between these countries be likely to end?''
To construct the new proxy, we use the super learner to make predictions for every directed dyad--year in the international system between 1816 and 2007, the range of years covered by the National Material Capabilities data.
We call the resulting dataset the Dispute Outcome Expectations data, or DOE.
The DOE data contains predictions for more than 1.5~million directed dyad--years.\footnote{
  About 19~percent of directed dyad--years contain missing values of the capability components for at least one country.
  We average across multiple imputations of the capabilities data to calculate the DOE scores for these cases.
  See the Appendix for details.
}

The DOE scores are naturally directed, since each dispute in our training data contains an initiating side and a target side.
However, many analyses in the international conflict literature (e.g., of dispute occurrence) use undirected data.
We calculate undirected DOE scores through a simple average of the directed values.
For example, to calculate the probability that the United States would win a dispute against the United Kingdom in 1816, we average its estimated chances of victory as an initiator (50~percent) and as a target (10~percent) to yield 30~percent.
If an analyst using the DOE data believed that the likely identity of an initiator in a hypothetical dispute were not a coin flip, she could take a different average of the directed scores to produce a more representative undirected score.

\begin{figure}[tp]
  \centering
  \input{fig-vs}
  \vspace{-2em}
  \caption{
    Comparison of the capability ratio to DOE estimates for selected major power dyads between 1860 and 2007.
    These plots use the undirected form of the DOE estimates.
  }
  \label{fig:vs}
\end{figure}

The DOE measures have two advantages over the capability ratio as a proxy for expected dispute outcomes.
First, they are direct measures of the quantity of primary interest to scholars of conflict: the probability that each state would win in a hypothetical dispute.
Although the capability ratio is a proportion, it cannot be interpreted as the probability of victory.
The ease of interpretation is particularly important for scholars who wish to control for expected dispute outcomes in a regression model.
The coefficient on a DOE score can be interpreted directly as the marginal effect of a state's chance of victory; the coefficient on the capability ratio cannot.
Second, as we have already seen, within the set of state pairs where disputes occur, the DOE measures are much better predictors of the outcome than the capability ratio is.
In short, they are superior proxies, and therefore are the appropriate choice for scholars who need an accurate measure of expected dispute outcomes.

The relationship between the capability ratio and DOE scores is weaker than we expected.
Across directed dyad--years, Country~A's capability ratio is correlated with the DOE estimate of the probability A Wins at $0.10$ and with B Wins at $-0.32$.\footnote{
  The two correlations are not the inverse of each other since DOE scores are ternary, with Stalemate as the third category.
}
As a close look at how the two measures might differ in particularly important cases, Figure~\ref{fig:vs} plots both measures for pairings of five major powers between 1860 and 2007.
The only country for which the capability ratio and DOE scores consistently tell the same story---namely, one of decline on the international stage---is the United Kingdom.
The two measures also track each other reasonably well in the United States--Russia dyad, with the US chance of victory taking a dip during the Cold War and increasing thereafter.
Notice, though, that the US's capability ratio relative to Russia's increases sharply after the Cold War, whereas the DOE estimate of its chance of victory only goes up marginally.
We see perhaps the most divergence in the China--Japan dyad, where increases in Japan's capability ratio seem to be associated with declines in the DOE prediction that it would prevail in a dispute.
In light of the DOE scores' superior predictive performance in the Militarized Interstate Disputes data, we are inclined to believe they dominate the capability ratio as a proxy for expected dispute outcomes.
Next, we test this conjecture by seeing if replacing the capability ratio with DOE scores in empirical models of international conflict improves their in-sample fit and out-of-sample predictive power.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "doe"
%%% End:
