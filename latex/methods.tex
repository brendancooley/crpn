\section{Building a Better Proxy for Expected Dispute Outcomes}
\label{sec:methods}

Our goal now is to squeeze as much predictive power as we can from data on states' material capabilities.
When prediction is the goal, ``black box'' algorithmic techniques usually outpace standard regression models \citep{Breiman:2001fd}.
So, to build our new measure, we augment traditional approaches with methods from machine learning.

\subsection{Data}

To evaluate the predictive performance of the capability ratio and then to build an alternative measure, we use data on the outcomes of international disputes.
We combine the National Material Capabilities data \citep{singer1972} with information on the outcomes and participants of Militarized International Disputes between 1816 and 2007 \citep{Palmer:2015hp}.
Our data consist of $N = 1{,}740$ disputes, each between an ``initiator,'' or Country~A, and a ``target,'' or Country~B.\footnote{
  See the Appendix for the data construction and coding specifics.
}
Every dispute outcome is either A~Wins, B~Wins, or Stalemate, which we denote by $Y_i \in \{A, B, \emptyset\}$, respectively.
Most disputes end in a stalemate, and victory by the initiator is more than twice as likely as victory by the target, as shown in Table~\ref{tab:mid}.

\begin{table}[htp]
  \centering
  \input{tab-mid}
  \caption{
    Distribution of the three dispute outcomes.
  }
  \label{tab:mid}
\end{table}

We model dispute outcomes as a function of the participants' military capabilities.
Our data source, the National Material Capabilities dataset, records annual observations of six characteristics of a country's military capability: military expenditures, military personnel, iron and steel production, primary energy consumption, total population, and urban population.\footnote{
  There are missing observations in the National Material Capabilities data.
  Consequently, about 17~percent of the disputes we observe contain at least one missing cell.
  We use multiple imputation to deal with missingness \citep{honaker_what_2010}; see the Appendix for details.
}
We also calculate each country's share of the global total of each component, giving us 12 variables per dispute participant.
The matrix of predictors has 26 columns: the 24 individual capability characteristics of the initiator and target, the standard capability ratio, and the year the dispute began.
The values of these predictors for the $i$'th dispute are collected in the vector $X_i$.

\subsection{A Metric for Predictive Power}

We face two challenges in evaluating a model's predictive power.
The first is to define a metric for predictive power---one that is appropriate to the task at hand and reasonably interpretable.
The second is to measure each model's ability to predict \emph{out of sample}.
Our main purpose, which is to measure the chances of victory for each side in a hypothetical interstate dispute, is inherently an out-of-sample prediction task.
We do not want a model that overfits the sample data at the expense of its predictive power when brought to new data.

As fortune plays a role in every military engagement, it would be impossible to perfectly predict the outcome of every dispute.
We therefore want a measure of predictive power that respects the probabilistic nature of militarized disputes.
Classification metrics like the accuracy statistic, also known as the percentage correctly predicted, do not fit the bill.
Instead, we employ the log loss, which is the negative of the average log-likelihood, as our metric for predictive power \citep[221]{Hastie:2009wpa}.
Let a \emph{model} be a function $\hat{f}$ that maps from the dispute-level predictors $X_i$ into the probability of each potential dispute outcome, $\hat{f}(X_i) = (\hat{f}_A(X_i), \hat{f}_B(X_i), \hat{f}_{\emptyset}(X_i))$.
The ``hat'' on $\hat{f}$ is there to emphasize that the form of the function has been learned from the data, whether by estimating regression coefficients or by a more flexible predictive algorithm.
The log loss of the model~$\hat{f}$ on the data~$(X, Y)$ is\footnote{
  To avoid numerical problems, very low probabilities are trimmed at $\epsilon = 10^{-14}$.
}
\begin{equation}
  \label{eq:log-loss}
  \ell(\hat{f}, X, Y)
  =
  - \frac{1}{N} \sum_{i = 1}^{N} \sum_{t \in \{A, B, \emptyset\}}
  \mathbf{1} \{Y_i = t\} \log \hat{f}_t(X_i).
\end{equation}
Smaller values of the log loss represent better predictive power, with the lower bound of~$0$ indicating perfect prediction.

We care mainly about the generalization error of our models---the expected quality of their predictions for new data that was not used to fit the models.
Our small sample size of $N = 1{,}740$ makes this tricky.
If we had a surplus of observations, we could use some suitably large number to fit our models and hold out the remainder to assess the models' predictive power.
But with as little data as we have, splitting the sample is ill-advised: we cannot hold out enough observations to estimate the generalization error precisely without harming the precision of the model itself.
So, to measure out-of-sample predictive power without losing data, we turn to $K$-fold cross-validation \citep[241--249]{Hastie:2009wpa}.
We randomly assign each dispute observation to a ``fold'' $k \in \{1, \ldots, K\}$, where we follow standard practice by setting $K = 10$.\footnote{Standard practice here stands on firm ground; \citet{molinaro2005} find that 10-fold cross-validation performs quite similarly to leave-one-out cross validation (the ``ideal case'' for cross-validation) without having to take on massive computational costs.  10-fold cross-validation also performs better than the .632+ bootstrap, split-sample techniques, and Monte Carlo cross-validation, particularly in smaller samples like ours.}
For each~$k$, we split the data into a ``test'' sample containing fold~$k$ and a ``training'' sample containing the remainder of the data.
We fit a model only on the training sample and then calculate its predicted probabilities for the data in the test sample.\footnote{
  \label{fn:nested-cv}
  When dealing with models with tuning parameters that are themselves selected by cross-validation, we choose tuning parameters separately within each of the $K$ iterations via another cross-validation loop.
  This nested cross-validation is necessary to keep our estimates of generalization error from being too optimistic \citep{Varma:2006ch}.
}
After repeating this $K$ times, we have an out-of-sample prediction for each observation in our data---one that was calculated from a model that did not see the observation in question.
We compare these predicted probabilities to the observed outcomes to estimate our models' generalization error.
Formally, the cross-validation loss of the model~$\hat{f}$ is the average out-of-fold log loss,
\begin{equation}
  \label{eq:cv-loss}
  \CVL(\hat{f})
  =
  \frac{1}{K} \sum_{k=1}^K \ell \left(
    \hat{f}^{(-k)}, X^{(k)}, Y^{(k)}
  \right),
\end{equation}
where $(X^{(k)}, Y^{(k)})$ is the data in the $k$'th fold and $\hat{f}^{(-k)}$ is the model~$\hat{f}$ fit to the data excluding the $k$'th fold.

Because it is measured on the log-likelihood scale, the log loss metric is hard to interpret on its own.
To ease the interpretation, we compare models' log loss to that of a null model, whose predicted probabilities always equal the sample proportions of each outcome.
The proportional reduction in cross-validation loss of the model~$\hat{f}$ is
\begin{equation}
  \label{eq:prl}
  \PRL(\hat{f})
  =
  \frac{
    \CVL(\hat{f}_{\text{null}}) - \CVL(\hat{f})
  }{
    \CVL(\hat{f}_{\text{null}})
  }.
\end{equation}
The theoretical maximum, for a model that predicts perfectly, is~$1$.
If a model predicts even worse than the null model---meaning, in essence, it is worse than random guessing---its proportional reduction in loss is negative.

\subsection{Modeling Dispute Outcomes}

Our task now is twofold: to assess the predictive power of the capability ratio and, should we find it lacking (as we do), to build a better alternative.

We model dispute outcomes as a function of the capability ratio via ordered logistic regression \citep{McKelvey:2010gv}.
To reduce skewness, we take the natural logarithm of the capability ratio.
The parameter estimates from the capability ratio model on the full sample appear in Table~\ref{tab:capratio}.
Although these results do not speak directly to the capability ratio's out-of-sample performance, they foreshadow why its predictive power is so limited.
The coefficient on the capability ratio is statistically significant but small relative to the cutpoints, indicating a substantively weak relationship.
Dividing the cutpoints by the coefficient, we see that we would need a logged capability ratio below $-13$ or above $+7$ to predict any outcome other than a stalemate.
These bounds lie well outside the observed range of capability ratios in the dispute data, which are bounded below by $-9.1$ (Palau--Philippines 2000) and above by $-0.0004$ (Germany--Panama 1940).
In other words, the capability ratio always predicts a stalemate within the sample.
This does not bode well for its out-of-sample performance.

\begin{table}[tp]
  \centering
  \input{tab-capratio}
  \caption{
    Results of an ordered logistic regression of dispute outcomes on the capability ratio using the training data.
    Because there are no missing values in the CINC scores, these estimates are identical across imputed datasets.
  }
  \label{tab:capratio}
\end{table}

We want a better model than what the capability ratio gives us, but we do not have a strong \emph{a priori} sense of what the data-generating process---the true relationship between material capabilities and dispute outcomes---looks like.
So we use tools from machine learning that are designed to predict well without imposing much structure on the data.
Ideally, we would select the predictive model that is best for our data, but there are too many algorithms to try them all.
To narrow it down, we defer to the machine learning experts on which algorithms are best.
We draw our set of candidate models from the top-ten list by \citet{Wu:2007ev} and from the best performers in the tests by \citet{FernandezDelgado:2014ul}.
After excluding those unsuited to our data,\footnote{
  Four of the algorithms named in \citet{Wu:2007ev}---$k$-means, Apriori, expectation maximization, and PageRank---are not suited for the prediction task at hand.
  We also excluded AdaBoost due to long computation time and naive Bayes due to poor performance in initial tests.
}
we end up with six predictive algorithms to try: C5.0, support vector machines, $k$-nearest neighbors, classification and regression trees, random forests, and ensembles of neural nets.\footnote{
  See the Appendix for full details of each method.
}
Each algorithm is widely used for prediction and can predict dispute outcome probabilities as a complex, potentially nonlinear function of the material capability components.
As a compromise between these flexible ``black box'' models and the rigid capability ratio model, we also test ordered logistic regression models on the capability components.

In the spirit of flexibility, we try each model with different sets of predictors from the capability data.
We examine four sets of variables: the raw capability components and the annual component shares, each with and without the year the dispute began.
All of our models allow for interactive relationships, so including the year of
the dispute lets the effect of each capability component vary over time.
With two sides per dispute and six capability variables per side, each model has 12 or 13 variables, depending on whether the year is included.
All told, we have 30 candidate models: four sets of variables for each of our seven algorithms, plus the capability ratio model and a null model used as a baseline.

We use cross-validation to estimate how well each of our candidate models predicts out of sample.
The final problem, once we have the cross-validation results, is to choose a model---the one we will use to construct an alternative to the capability ratio as a measure of expected dispute outcomes.
It is tempting to simply pick the model with the lowest cross-validation loss.
We can do even better at prediction, however, by taking a weighted average of all the models.
We use the super learner algorithm \citep{vanderLaan:bz} to select the optimal model weights via cross-validation.
Given a set of $M$ candidate models $\hat{f}_1, \ldots, \hat{f}_M$, we select weights $\hat{w}_1, \ldots, \hat{w}_M$ to solve the constrained optimization problem
\begin{equation}
  \label{eq:super-learner}
  \begin{aligned}
    \min_{w_1, \ldots, w_M}
    &\quad
    \CVL \left(
      \sum_{m=1}^M w_m \hat{f}_m
    \right)
    \\
    \mbox{s.t.}
    &\quad
    w_1, \ldots, w_m \geq 0,
    \\
    &\quad
    w_1 + \ldots + w_m = 1,
  \end{aligned}
\end{equation}
Our final model is the super learner, $\hat{f} = \sum_m \hat{w}_m \hat{f}_m$.
Each individual model is a special case of the super learner, with full weight $\hat{w}_m = 1$ placed on a single $\hat{f}_m$.
Hence, by the cross-validation criterion, we should prefer the super learner over any individual model.\footnote{
  \label{fn:sl-bias}
  As usual when selecting tuning parameters via cross-validation, the value of equation~\eqref{eq:super-learner} is not an unbiased estimate of the generalization error of the super learner.
  Nested cross-validation is computationally infeasible for the super learner, so we calculate the bias correction recommended by \citet{Tibshirani:2009tz} to estimate its generalization error.
}  That said, the super learner does provide the capability ratio with an opportunity to defend itself; should it earn a high weight, then our costly enterprise may not be worth the effort.

To summarize, we fit and cross-validate $M = 30$ candidate models, then combine them into a super learner that we will use to construct a better proxy for expected dispute outcomes.
The biggest downside of our approach is that the results are not easily interpretable.
Because the super learner entails averaging a large set of models---some of which, like random forests, are themselves difficult to interpret---it gives us no simple summary of how each predictor affects dispute outcomes.
This is not a problem, given our aims.
Certainly, we would not recommend the super learner as a means of testing hypotheses about the determinants of dispute outcomes.
However, our goal is not to test a hypothesis---it is to construct the best proxy possible for how a dispute between two countries is likely to end.
In this context, it is worth sacrificing interpretability for the sake of predictive power.

\subsection{Cross-Validation Results}

We now turn to the cross-validation results, which are summarized along with the super learner weights in Table~\ref{tab:ensemble}.
As the in-sample analysis hinted, the capability ratio is indeed a poor predictor of dispute outcomes.
Its proportional reduction in loss is~0.01, which means its predicted probabilities are just 1~percent more accurate than the null model.
This number is not encouraging, but what matters even more is whether we can do better.
A glance at Table~\ref{tab:ensemble} confirms that we can: all but one of our 28 alternative models have greater predictive power than the capability ratio, many of them considerably better.
With these results in hand, we feel comfortable dismissing the capability ratio as a suboptimal proxy for expected dispute outcomes.

\begin{table}[tp]
  \centering
  \input{tab-ensemble}
  \caption{
    Summary of cross-validation results and super learner weights.
    All quantities represent the average across imputed datasets.
  }
  \label{tab:ensemble}
\end{table}

As we expected, the super learner ensemble performs better than any of the candidate models from which it is constructed.
The ensemble's proportional reduction in loss is about 23~percent, or four percentage points better than the best candidate model.
Even after we apply a bias correction (see footnotes~\ref{fn:nested-cv} and~\ref{fn:sl-bias}), the super learner's predictive power is still the best among our models.
Looking at the weights, what stands out is how few models are substantial components of the super learner: just five models have a weight of at least 5~percent.
More generally, while models with lower generalization error tend to receive more weight, the relationship is by no means one-to-one.
We see this because the ensemble prefers not only predictive power, but also diversity.
Different classes of models have different blind spots; the more diverse the ensemble is, the more these blind spots are minimized.
A model that looks bad on its own might still merit non-negligible weight in the optimal ensemble if it captures a slice of the data missed by the models that are best on their own.

\begin{figure}[tp]
  \centering
  \input{fig-oof-pred}
  \vspace{-2em}
  \caption{
    Ternary plots of out-of-fold predicted probabilities according to the capability ratio model and the super learner.
    Each predicted probability is calculated by fitting the model to 9/10 of the data, not including the observation in question---an approach that simulates true out-of-sample prediction.
  }
  \label{fig:oof-pred}
\end{figure}

The super learner predicts dispute outcomes much better than the capability ratio does.
As we have just shown, the capability ratio only improves by~1~percent on a null model, whereas the super learner gives a~20~percent improvement.
For another illustration of the difference in predictive power, see the plots of out-of-fold predicted probabilities---the ones we use in cross-validation---in Figure~\ref{fig:oof-pred}.
Under the capability ratio model, all but a handful of disputes are predicted to have an 80--90~percent chance of ending in stalemate.
Seeing how narrow the capability ratio's predictive range is, it is little surprise that it barely does better than a null model at prediction.
Conversely, the super learner makes much better use of the material capability data.
Its predictive range is greater, which in turn allows it to achieve a stronger, though hardly perfect, relationship between predicted and observed outcomes.

\subsection{Implications for International Relations}

Our main focus is on developing a proxy for relative power that predicts the outcomes of militarized disputes, and predictive approaches like ours are not optimal for testing specific hypotheses \citep{Shmueli:2010jd}.
Nonetheless, we can glean from our results a few important insights about the nature of the relationship between capabilities and power.
The first is that there \emph{is} a relationship---that variation in dispute outcomes is associated with variation in the disputants' raw capabilities.
This finding contrasts with previous studies concluding that military capabilities do not affect dispute outcomes \citep{Maoz:1983cw}.
The problem is the use of the capability ratio to measure material power.
The capability ratio is at best weakly related to dispute outcomes, so studies that rely on it will conclude that material power is unimportant.
We find, however, that material power is related to dispute outcomes but poorly measured by the capability ratio.

But material power is not all that matters.
Even after an intense, diverse predictive effort, we are only able to explain about 20 percent of the variation in dispute outcomes with material capability variables.
To some extent this reflects the inherent unpredictability of military affairs; we would never expect to predict outcomes perfectly.
We suspect, however, that we could better predict dispute outcomes even better by conditioning on even more observable indicators.
That is a task for future work, as the purpose of this paper is only to develop a proxy for the material components of relative power.

A second important finding is that the determinants of material power change over time.
This conclusion may sound obvious, but it raises the question of why international relations scholars continue to use a proxy for power that assumes the relationship is unchanging.
The simplest way to observe that time matters is to compare the predictive power of the models with and without the year variable: in 13 out of 14 cases, the model that includes time predicts better than its closest time-less counterpart.\footnote{%
  The difference in log loss is statistically significant (paired $t = -3.25$, $p = 0.006$).
}
The improvement in performance does not just reflect the fact that the distribution of dispute outcomes has changed over time (though, to be clear, it has).
All of our models that include the year of dispute allow for the effects of each individual capability component to vary with time.
For example, in our random forests on the components and year, every single tree contains at least one node that splits on a capability component whose parent node splits on the year of dispute, indicating a capability effect that is time-dependent.
In fact, in about 27~percent of trees, the initial split is on the year of dispute.

Finally, our results show that a country's material power cannot be summarized in a single index.
Our ordered logit models construct a flexible index that allows for different weights on the components, variation over time in those weights (in the model with year included), and different weights for initiators and targets.
Nonetheless, even the best ordered logit model has less than half the predictive power of our flexible ensemble.
What this means is that the relationship between each capability component and overall material power is conditional, not absolute.
The usefulness of a particular component may depend in part on the composition of the opponent's capability holdings.
The super learner results are too complex to allow us to dig into the specifics of these interactions, but they alert us to their existence---and to the limitations of monadic indices as a measure of power.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "doe"
%%% End:
